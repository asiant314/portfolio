[
	{
		"question": "You are new to a project and there's no persona or customer segmentation information available. How would you go about developing personas that are representative and practical as design tools?",
		"answer": "Applies a mixed methods approach, involves both ethnographic work in addition to surveys, market segment data, usage logs, Understands difference between market segments and personas. Discusses approaches for dissemination, usage of persona",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "Your eng team is paralyzed as they have so many features they want to build for the next quarter, but only enough resources to do a couple. Design a study to help them prioritize what they should work on, to drive the most conversion.",
		"answer": "Talks through different feature prioritization approaches (Kano, conjoint, max diff, ratings). Includes initial formative/foundation research to identify user needs.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "You are evaluating 3 different UI text options for an app download screen. You can test these options via a series of moderated usability session or via an online survey tool. How do you determine which approach you pick? For the survey based approach, how many people do you end up surveying? Would you show them each of the 3 variants? Only one?",
		"answer": "Tradeoffs in study design, usability to iteratively improve, identify issues in variants at the cost of time and resources. Surveys to scale for larger audiences, quantitative comparisons across the designs. Survey sampling and study design (within vs between subject tradeoffs).",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "In usability studies, how would you describe your moderation style? What is your best technique to capture behaviors and reactions most representative of how users will do in the real world setting? Participants don't talk? Participants that talk non-stop? Participants who think they know what you want to hear?",
		"answer": "Focus on behavior, less so on subjective statements. Moderation best practices for moving studies along at a good pace while gathering high quality data. Making sure participants are comfortable, building up rapport.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "After launching new designs for a product, your long-time users had a strong negative reaction to the initial designs. Design a research plan to study and understand the reaction. You are particularly concerned with separating out change aversion from core problems.",
		"answer": "Focuses on tracking sentiment and change aversion. Recognizes that there is some time before stabilization. Baseline before and after comparisons with satisfaction tracking. Looks at only new users to control for change bias",
		"tag": ["tag3", "tag4"]
	},

	{
		"question": "Suppose you are joining a relatively young team yet to have the luxury of having a dedicated user researcher. What are some strategies you would employ to help the team better leverage your core competencies?",
		"answer": "Focus on building rapport with team, defining processes, building relationships. Encouraging participation in design activities, research. UXR 101 type of seminar. Regular brown bags to demonstrate impact of research",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Talk through your typical process and timeline for conducting a more tactical usability study? How would this change if you were given half the time to complete the research?",
		"answer": "Explores lightweight usability approaches (cafe studies, remote usability, unmoderated usability, internal studies, advisory boards). Recognizes potential tradeoffs with these approaches",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How would you evaluate a concept for a new product or service that has not yet been seen in the market, in other words there is no direct peer or competitor to compare it.",
		"answer": "Ethnographic research to understand user needs, inform concepts. Competitive review to understand what is currently out there, though caveat being you don’t want to become too fixated on existing designs such that it impedes innovation (ex/ in focus groups, I’ve seen occasions where users often reference other products and features that they found exciting). Concept study to explore early ideas. Trade-off between concept studies and survey based approaches such as Kano/MaxDiff. Concept studies are great at diving deep, though it’s time consuming (creating concepts) and you are often limited in how many you can share. You can use a storyboard to describe the concept holistically within a flow. Surveys allow for greater scale (number of concepts, number of participants and subsegments of users), since these responses are quantified, you can get more nuanced comparisons and rankings of concepts",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "What have you found to be the most effective ways to communicate your research findings and recommendations? Provide examples.",
		"answer": "Range of approaches depending on audience. Domains international research across 5 countries. Crafted two different presentations depending on audience. First one was to the entire xfnal team (eng, business, leadership, ux). This was a broad audience, and my goal here was to increase awareness and empathy about he international adoption of our product. Decided to do first send out a broader quiz to the team, with questions based on our research findings. Then during the presentation, I showed the results of the survey responses along with the correct answer. This was a great way to demonstrate how the team’s initial conceptions may or may not have aligned with our actual research findings. Added a bonus android figurine for the the top quiz scorers as a bit of motivation also. Second approach was a smaller group of 5-6 stakeholders (PMs, business leads, eng leads). Goal here was different, instead intent was to drive action based on the findings. I reiterated some of the high-level points from the research but I left this presentation more open-ended allowing for discussion and brainstorm of ideas.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Whether or not the idea was eventually adopted, describe a situation in which one of your research efforts resulted in you having an interesting or novel product or feature idea. ",
		"answer": "I was conducting a shop-along type of exploratory exercise to understand how users select domain names for their business. The goal of this research was to generate ideas for iteration of our search process within Domains. I provided users with a re-imbursement, watched them work through the process of finding a domain name. One area that stood out in this walk through was the emphasis users placed on social media handles. For them, a business wasn’t just the website name but also an instagram, twitter identity, business listings on Facebook, Yelp. The underlying need for users was to have a cohesive brand for their digital presence. The idea that came out of this was… could we also check social media availability in addition to domain names as part of the search experience?",
		"tag": ["tag3", "tag4"]
	},

	{
		"question": "Describe a difficult situation you encountered with a product team (e.g. strong opinions and personalities).  How did you deal with this situation?",
		"answer": "On the AWS marketing team, there tended to be some tension between the marketing analytics team (responsible for A/B testing) and the UX team. I received a number of questions about the generalizability of my insights from qualitative work, mostly along the lines of… we haven’t really seen this as an issue in our log data. Underlying this is a question about small sample sized qualitative research. Early in my career I would take a more defensive approach towards it and go into thesis defense mode, talking through the rigor of the research approach. I’ve shifted away from that  and instead start by trying to understand where the log data is coming from. I reframe it as another dimension that can be used to tell a cohesive story. Often, I find that it’s not really in conflict with the qualitative insights and when you dig deeper into the log data, complemented with the qualitative you get a more thorough understanding. I also talk through how the questions being answered by these data sources are different. Logs -> more about how often and what is happening, while qual -> why it’s occurring, the intent behind actions. As an example, let's say you introduced a CTA and the engagement rate is incredibly low… is it an issue of discovery or utility? ",
		"tag": ["tag3", "tag4"]
	},

	{
		"question": "Describe a situation in which your research plan or insights were challenged by stakeholders. How did you deal with this situation?",
		"answer": "Worked really hard on a feature redesign within Google Domains, informed by iterative usability studies, and grounded by early foundational work. As part of the post-launch process, we conducted A/B testing and there were some serious considerations to take into account. Both an influx of negative feedback and a reduction in A/B metrics. Some of this I’ve experienced before in other products and I pointed out how often with redesigns there may be an initial negative reaction. Some of the open-ended feedback related to aesthetics, and though helpful to note should also be balanced by change aversion. One approach I proposed to tease out some of this impact is to actually give the metrics some more time. I also suggested breaking the metrics down into new users vs old users to help control for change resistance (there new users would not be impacted by the old design).",
		"tag": ["tag3", "tag4"]
	},

	{
		"question": "Tell me about a time you had to deliver negative feedback to a team member.",
		"answer": "I was mentoring a researcher on my team and had given him the lead on a research project. During the course of the project, there were a few instances where I didn’t think he communicated effectively to the stakeholders. In the first, the researcher did not loop in stakeholders regularly and so they were surprised when a study announcement was sent out a few days before the first participant was scheduled. This left the stakeholders off-guard and felt unprepared for the research. This led to a delay of the research and a rescheduling of the participants. In the second instance, I noticed that the stakeholders (design/pm) came out of a research meeting somewhat frustrated. In leading the meeting, the researcher was deferring to design/pm to often make research decisions (how many concepts should we test, how should we word these concepts). I followed up with design/pm to verify that the observations were correct before talking with the researcher one-on-one. I framed the feedback in terms of action and impact versus focusing on the researcher themselves. For example, “I noticed that in the meeting stakeholders were often asked about research design decisions. It’s great to include stakeholders in this planning process but leaving it that open-ended places a lot of burden on the stakeholders to also be researchers. Oftentimes they are turning to you as the expert. An alternative approach could be to provide a stronger point of view on what you would recommend given your expertise and if the stakeholders disagree or have alternative opinions, this would then be a great point of discussion.",
		"tag": ["tag3", "tag4"]
	},

	{
		"question": "What are the weaknesses of personas? How do you overcome those weaknesses?",
		"answer": "How are they to be used? Can we get alignment on high priority persona? Personas are a great resource when it comes to providing a frame of reference for designers, developers, product managers. They can all reference back to the persona when trying to frame key decision points. In addition, it creates a realism and sense of empathy when you can refer back by name to the persona, place that persona within a scenario, and walk through how they may perceive of a feature. What goes into creating a persona can often lead to limitations in use. For example, I’ve seen instances where personas are mainly market segments, consisting of demographic profile information. There is no backstory that drives the needs of the persona. On the other hand, I’ve seen personas made with dubious supporting evidence. Despite the story telling nature of crafting personas, these are based on a certain level of evidence. You use a mixture of focus groups, interviews, contextual inquiries, observational studies, market segmentation data to build up an understanding of the user groups. From this you can craft the personas for distinct groups. I’ve found that you get the most out of creating the personas since this involves a fair amount of research and data gathering. As you transition that into the persona story, unfortunately some of the information will get lost. A third party reading the persona, though it may be helpful, loses out on the context driven by persona creation. I’ve found in this case that having designers involved with the persona creation process pays dividends. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How do you ensure that the research your group performs is impactful?",
		"answer": "I approach this from two ends. First is identifying the research that will make a strong impact. As researchers, we have strong insight into when research could benefit the product direction. In interacting with users regularly, we can pull out overarching needs and pain points that aren’t necessarily scoped to a particular feature. I think it’s part of our role to insert ourselves when we see that decisions are made without a strong grounding on user insights. When thinking about what makes a research effort impactful, I look at: what problem is being solved for the users (both in terms of frequency and severity), the novelty or degree of risk associated with the solution, the impact on the business, and level of engineering effort. The second end is what happens after conducting the research. I find that one of the most overlooked parts of being a researcher is the ability to tell a compelling story. Afterall, in the end, you want your work to drive action. I’ve applied a number of different approaches, and it varies very much by audience. For example, after doing some extensive international research: large team wide group I presented as a Q&A type of exercise (what do you think you know, what did we find out). But then in smaller working group, it was focused on highlighting the issues and opening up discussion for how to address them. The goals of the presentation were different. When presenting at a leadership level, I try to understand what questions they currently have of the space, how they respond to different insights (ex/ are they very much focused on data, stories).",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "With what methods do you have experience analyzing qualitative data? (Content analysis methods). Can you tell me about a particularly challenging bit of qualitative data analysis you’ve done in the past?",
		"answer": "I often apply a bottom up qualitative thematic analysis approach. I start by reviewing the data and developing a preliminary codebook which I then go back to code the sessions. From the codes, I extract out common themes. For the full rigor, I would do this with another researcher to collaboratively define the codebook, conduct the coding independently and then check for inter-rater reliability. In practice, this is often too time consuming and it’s just a one-person approach. I ground my insights on user behavior and needs rather than purely what users say. Often times users provide feedback such as, oh I like design A over B, but what’s more interesting is diving into why, what needs are being met/unmet by the designs? And their behaviors which they often don’t realize they are doing. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "What research methods are you most expert in? What are their strengths and limitations compared to other methods?",
		"answer": "My strengths are focused on applying mixed methods approaches towards research questions. I get most excited when I get an opportunity to blend both quant and qual work in order to provide a more holistic understanding of a research area. Some examples of this include: persona development for domains (quant: log and segmentation analysis, qual: user interviews), GMB product integration (quant: log analysis, qual: user interviews), feature prioritization studies (quant: maxdiff/Kano, qual: concept study). For me it’s finding the right approach towards answering a research question. I use the quant approach often to understand existing behaviors within product (log data), or to scale responses for more targeted comparisons (survey). The qual methods provide me with greater depth and understanding of user needs. There’s often an interplay between the two approaches; for example I’ve used interviews to then inform surveys.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Think about an app you like to use. Suppose the product manager tells you that he wants you to find the top 10 UX issues. How would you go about this?",
		"answer": "First start is to set criteria for what is defined as a top UX issue. Does this include the number of people impacted, how much it impairs task completion, impact on user perceptions, business ramifications? Once that is better defined, then we have a more consistent frame to compare issues. Next I would look at the different data sources we have available for user insights. Often UX teams already have a repository of research results, there’s also customer support tickets, and maybe even open-ended feedback in product. I would dig through that to start grouping and identifying themes for UX issues. At the same time I would also think about the inherent biases in these sources. For example, those who submit support tickets may face a different level of severity for their issue, vs someone who leaves open-ended feedback. And both of these groups often represent a particular subset of users who contacted our product. To me, these provide different lenses into user issues and that was why it was important to have a rubric ahead of time to define what it means to be a significant issue. Once the themes have been identified, I’d use the previously established rubric to start ranking the issues. In the case that I was coming into it a complete vacuum with no preliminary data then I would consider this a more long term project. You have to get that initial set of studies (plural) going to see the big picture issues that come up. Once I have a preliminary list of issues, I’d design a research study to validate them. This could be in the form of a prioritization survey such as Kano, MaxDiff. If the list is small enough, I might consider a qualitative study with artboards demonstrating the issues on the list. Then I would follow-up with questions around whether this is an issue they’ve faced and how impactful of an issue it might be for them. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How do you handle it when people are skeptical of the value of usability research?",
		"answer": "I come across a number of articles related to this. One good resource is the Medium article ‘The Secret Cost of Research’. Her main point is that it gives the different members of a project a shared basis for decision-making, grounded in evidence rather than in authority or rhetorical appeal. Coming from a health informatics background, the example that always comes to mind for me is the Cedars Sinai implementation of a computerized physician order entry system. Great team, brilliant work, implemented this system for physicians to input drugs, automatically detect adverse interactions -> rolled back after 2 weeks as physicians refused to use it. Too many false alerts, did not align with their workflow, complexity of the system. Couple approaches to increase buyin… have people observe the sessions. It’s amazing the feedback I’ve received as stakeholders sit in for the first time and get a chance to see users interact. I also try to speak of it from a business perspective. Test early and you are able to identify key issues without sinking resources into development. You also get to try out different iterations. It also always goes back to the user. Having a great user experience will improve your key performance indicators, whatever they may be.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Under what situation would you use focus groups? Usability sessions?",
		"answer": "Focus groups consist of a small group of people sitting together discussing their thoughts and opinions on products, past experiences with them, reactions to new concepts. Really good for getting a sampling of users’ feelings and opinions about things. Good for abstract wants, needs, and likes (is the idea, value prop on the right track?) However it’s no good at learning whether or not your product works and how to improve it. Usability tests involve watching one person try to use something so that you can detect and fix things that confuse or frustrate them. Main point here is you are watching as they use the product rather than listening to them talk about it.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How would you determine a metric for engagement?",
		"answer": "I would first start off by teasing apart what it means to be engaged. Always hesitant to jump right into defining these constructs, especially since there is a subjective, qualitative component to them. My thoughts on engagement are that it involves both depth and quality of interaction with your feature of interest. One way we could validate this is to interview users, have them walk through a few apps, sites that they find engaging. Dig into what makes it engaging. I know for clickstream data, engagement gets thrown around a lot as number of people who view a page and don’t bounce off, or stay for x period of time. Always a bit critical of that approach since you don’t know if they are on there because it’s interesting, fun, or if they are frustratingly trying to find a piece of info that doesn’t exist. A couple ideas come to mind for measuring engagement. One would be a questionnaire approach. In that case I would go to the literature to see what people have developed previously to quantify this construct, check that it has been validated. I could consider coming up with a questionnaire of my own… but that’s a fair amount of work b/c you want to make sure you are measuring the right thing in a consistent manner and interpreted properly by respondents. Alternative approach is a more qualitative descriptive. Review usability sessions, code it for engagement. Maybe double code for inter-rater reliability.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Asked if the team want to add a feature, what research process and activities will I do, what questions I will ask for the usability testing etc..",
		"answer": "Start by understanding where this request is coming from. What potential problems prompted the decision to incorporate the feature, what is it intended to solve, has there been existing research that informed this, perhaps as a baseline of some form? What about the audience, are you targeting a certain group, is the impact across all user types for your product, perhaps it’s a feature for advanced users? Are there any preliminary concerns with introducing the feature? What do you consider a success? Once I understand the needs from the stakeholders along with the business requirements, then I will look at the study design. This really depends on the questions that stakeholders have. If it’s more exploratory and generative in nature, I would consider doing observational work, contextual interviews, or lab studies to see how users are currently interacting with the product. If that leg work has been completed and we’re at the stage of ideation and iteration, I would look at doing a usability study to inform the design, make sure we are going in the right direction without investing significant resources into development. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "They asked me to define some metrics for measuring fun and satisfaction for a mobile maps product. How would I collect the data etc...",
		"answer": "Initial thoughts on this are that, fun and satisfaction are challenging metrics to quantify. There’s a personal subjective element to them. For example, different individuals have differing thoughts on what is fun. The first approach for me would be to clarify what these constructs refer to, both from a stakeholder perspective and an end user perspective. One approach to doing this is an interview with users as they try out some of their favorite apps. I would want to dig into, over the course of the interview, what makes the app fun and satisfying. For example, is it due to elements of lightheartedness, is it because they are able to accomplish a goal quickly, is it because they discover secondary benefits while completing a task? That gives insight into the underlying elements that define fun and satisfying. I know there are clickstream data approaches available to get at some of this information. For example, I’ve seen people measure duration on a page as a proxy for engagement, or completing a task such as sign-up successfully as a measure of satisfaction. I’m always a bit hesitant about reading into that heavily, primarily b/c you lose context for why users are interacting in such a way, in particular you don’t have much insight into their intent and their reactions afterwards. I would take a mixed approach, first qualitative in nature where I have them use the product across a set of open-ended tasks. I would later go back an qualitatively code for instances of fun and satisfaction based on a rubric. I would also look at administering a questionnaire at the end of the session designed to measure these constructs. For the questionnaire approach, rather than coming up with a set myself, I would go to the literature to see what has been published. These tend to be validated in some way, though as a secondary resort I would design the scale on my own.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "The only one that stumped me was what's the difference between a Persona and a Market Segment.",
		"answer": "Market segments refer to different groups of customers based on predominantly demographic, quantitative data. It’s an approach where you gather characteristics of your customer, use it to target different products, services, or messages. It describes the composition of your users. Personas integrate both market data along with interviews, focus groups, usability studies, (more qualitative approaches) to define a profile of the customer. This profile contains a story that sets up the context of the persona, their motivations, needs, goals. It’s a fictional character(s) rather than an averaged group of customers. Personas are used predominantly to assist in the design process as they help designers empathize with the customer story. Designers are not necessarily customers; personas help provide this contextual switch. Personas also don’t have to represent all of your segments.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How I would protect a user's content if they chose to use their personal email during the experiment?",
		"answer": "For those types of situations, I would want to avoid altogether in the first place if possible. For example, if over the course of a study, I know that login may be needed, I will provide users with login credentials as part of the instructions. That way we don’t get into the uncomfortable situation of them putting in personal information. There are cases where it might be helpful to have them log into their account. For example, in AWS, it helps to understand how users may have set up a view of the console dashboard or to provide an understanding of the architectural setup. In these situations, I am very explicit in making it an optional choice for the user; they are under no obligation to share though the NDA is mutual in case they are worried about confidentiality. As they login, I stop the screen sharing so that they are able to type in the information confidentially, get to where they need to go without disclosing any other information.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Explain the P value as if you were talking to your grandmother.",
		"answer": "Talk about how you are using a sample to estimate the true value of the population. It’s not going to be a one liner unfortunately. In order to understand p-values, I think you need to have context for hypothesis testing. In short you have a null hypothesis, a statement of assumed truth (ex/ men and women are of equal height). You have an alternate hypothesis that you want to test against the null (ex/ men and women aren’t of equal height). You go and collect a bunch of data on heights of men and women. The p-value is the probability that you have such a sample, assuming that your null hypothesis was true. If the p-value is significantly low, you would conclude that, something like that happening by chance is so unlikely that the null hypothesis is probably false.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How do you know when a project is done?",
		"answer": "Good question, it’s never about doing a study sending out the report, holding a debrief and then you are set. Biggest challenge at my current position is the follow-up. We identify issues, some stakeholders are better than others at addressing them… but still there is quite a fair bit of pestering that needs to be done. I’m looking at putting together a ticketing system that catalogues the issues, assigns an owner to them, and allows tracking of the issues. There’s always a balancing act also between level of effort and severity of an issue; have to be persistent about the issues that you find critical. I also think we focus too heavily on the launch of a feature and don’t effectively track landing outside of an A/B experiment during the first few weeks. That’s a bit of a disservice b/c you get great insights into how users’ behaviors have changed after the introduction of a feature that you can monitor with logs, get open ended feedback from. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "tell me how would you design and lead a usability test for a product that is going to be launched in 6 month. give me detailed method and user recruitment plan.",
		"answer": "Start by meeting with stakeholders, be it product managers, developers, designers. Get a better sense of the current state for the product (is it early prototype, under development, pretty close to complete, beta testing for bugs). This makes a difference in terms of defining what the study plan will be. I also want to understand what the concerns are from the stakeholders, any assumptions they are making about the user, what existing research has been completed, what questions do they have about the product. This is usually my kickoff meeting, from which I put together the research questions. Figure out what the target user profile will be. This allows me to put together a participant screener. At AWS I’ve got a few approaches towards recruiting… ranges from different user profiles we have available, intercept requests on marketing pages, and third party screeners. Regardless this is generally the bottleneck and so I would want to get it started as early as possible (2 weeks ahead) . Easiest for me is to put together a screener, send to a third party recruiter. While the recruiting is underway, I would put together the task list while working with stakeholders. Set deadlines and expectations for development/design. In particular, what is the state of the product available for testing. Have a review of the task list, make sure everyone is on the same page, put time on people’s schedules for the testing. I recognize that people’s schedules are busy, but set the expectation that they observe at least half of the available sessions. As prep for the study, I would do a pilot run if time allows. Iron out kinks for task list, identify any gaping holes for product functionality, also make sure all the logistics of the study are set (equipment, recording, audio…). Study week itself isn’t too bad, probably one of the more enjoyable stages since you are interacting with users. Hold a short debrief after each session, ask observers about some of the issues they saw, make adjustments on the fly if it’s consistent. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Take me thorough how you do descriptive/inferential analysis for quant data?",
		"answer": "My first thought is to understand the type of data we have, the questions we’re looking to get answered, and whether statistical analyses are appropriate for addressing them. For example, if it’s small n data, richer in qualitative insights, I don’t see value in focusing on statistics for it (or even really reporting the N), instead I focus on ‘some’, ‘most’, ‘all’ descriptors. In the case where it is yes, next step is to figure out is the data that we are collecting (ordinal, categorical, continuous). Followed by the study design and research questions. For example, is it a within subject study, is it between subjects. What are the hypothesis being tested with the data. This makes a difference later on in the choice of statistical tests, paired vs unpaired, while the data type makes a difference on parametric vs non-parametric approaches. In short for me, choosing a statistical test is all about understanding the assumptions made of the test and whether or not your data meet those assumptions. The other issue I try to catch is over testing and cherry picking. This is especially true for web optimization related tests where you have a fair number of metrics available. The issue is that, if you look around for metrics of significance here and there without predefined objectives, you open yourself up to false positives. As a result, I make sure that we have hypotheses defined before hand, with corresponding metrics to test and desired power. This defines the sample size and a stopping point for the tests.I don’t do anything too fancy when it comes to testing. Truthfully I always forget the names of the tests but that’s where a quick Google search helps. My decision point is: descriptive vs inferential, parametric vs non-parametric data, paired or unpaired data, and comparisons between 2 groups or amongst more than 2. The ones that come up most frequently for me are t-tests (paired, unpaired) for continuous data, ANOVAs for comparing across multiple groups, Chi-squared for multiple for groups. For non-parametric data, go with Chi-squared, Mann-Whitney, Wilcoxon (paired). ",
		"tag": ["tag3", "tag4"]
	}

]