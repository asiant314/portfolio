[
	{
		"question": "Can you tell me about a mistake you made recently? What did you do to correct it? If you had it to do over again, how would you have done it differently?",
		"answer": "I worked on a project to revamp the search experience for finding a domain name. We applied a pretty rigorous UCD process, with formative and evaluative research to validate our design direction. However, as we were nearing launch, there were concerns raised by BD and legal about an element of our design. It was an issue where a majority of the cases would be fine but a select few might lead to negative optics from the broader community. It was something we didn’t catch during the research and scoping of the project but led to a shift in direction and a decent amount of throwaway work. In reflecting back on this, I think there were two issues that we could have been better prepared for, 1) making sure there was earlier and more frequent visibility into our work, 2) a more structured approach to account for change. I led a retrospective to discuss these two issues with the team and we implemented changes within our process that added rigor and efficiency. To address 1, we created another review outside of a midpoint check-in and expanded audience to include broader XFN scope. For issue 2) allocating more time to stress test, and to ground any re-prioritizations based on a framework of CUJs and success metrics in a product requirement doc ahead of time.",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "Describe a difficult situation you encountered with a product team (e.g. strong opinions and personalities).  How did you deal with this situation?",
		"answer": "When I first joined the Google Domains team, I worked with PM, eng, and UX design to drive an update to our domain search experience. It was a nearly 9 month revamp of the search experience and it was launched with strong success as measured by A/B metrics. However, over the course of this engagement, I observed that there were still areas where we could improve as collaborators. It was less about the outcome, which I thought we shipped a strong user centered experience, but more about the processes for getting there. I led a retrospective for us to reflect on this project. This is a term borrowed from engineering practice where retrospectives are often done in response to unexpected (and often negative events). I reframed this as a retrospective to identify opportunities where we could better improve our processes and collaboration. The audience for this included tech leads, engineers, PM and UX. One thing I wanted to be very mindful of was providing a safe environment for discussion without creating blame. To structure this meeting, I went back and replotted the timeline for our engagement on the project, from initial scoping through to launch and measurement. Along there I identified key milestones, ex/ brainstorming sessions, design reviews, implementation. I asked stakeholders to brainstorm offline, using this timeline as a refresher, areas that they thought worked well/poorly within this engagement. This was aggregated into a document and for the meeting, I synthesized them into key themes. From there we held a discussion on how we might better address the issues that came up.The outcome from this was a summary of both areas that worked well (which we made sure to retain within our processes) and a set of changes that we’ve implemented moving forward. For example, one them we identified was improving efficiency in collaboration (running notes for xfn syncs, documenting decisions made and their rationale -> addresses the churn and revisiting of ux decisions), another around having a more defined and aligned project plan (this led to introduction of a larger team design review), a third was about accounting for change (this led to allocating more time for stress testing across form factors, grounding any re-prioritization across a set of key user journeys and user persona).",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "How do you ensure your research is ethical and protects participants?",
		"answer": "1) In structuring the research, being aware of PII (personal identifiable information) and ethics. Participants are sharing information with us and we should be stewards of this information. Not asking for sensitive questions, always giving participants the space to not answer if they don’t want, managing the data from the sessions in a sensitive manner. For example, removing PII, blurring out face/name, having a data retention policy in place. 2) Being transparent with participants about what we are doing in the research session. Having them complete an informed consent, making sure they understand the consent form and giving them opportunities to ask follow-up questions if needed. If I am recording, even with signing a consent, confirming with participants that it’s okay to record a session or to have others join for observation.3) Turning to legal or UX infrastructure for help when needed. For example, when conducting research with minors, thinking through how to ask questions related to A11Y, unsure about potential sensitive topics.",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "Describe a time that either a part or an entire line of research that you designed or executed failed. What was the failure due to? What did you learn from the failure? Did you learn something valuable from the failure, and if you could go back and do things over again, would you do them again in the same way or differently?",
		"answer": "On Google Domains, I initiated a Quarterly Customer Review with my leadership team as an opportunity for us to go over a summary of the insights that we learn about our users across both ux research and customer support. The intent of this review was to surface the pressing issues users struggle with, and highlight how our user metrics have changed over time. I collaborated with the customer support team to put together this review and followed up with leadership after presenting to get their feedback (especially given this was the first instantiation of the QCR). One consistent area of feedback was around the level of content in the review. We covered a fair bit of insights across different topics and there was a lot of good discussion. However it was broad in scope and didn’t give stakeholders a good opportunity to dive deep and identify actionable next steps. In reflecting on this, I think having the stakeholders in the room is a great way to align on decision making and I reframed the QCR to be less about a share-out of insights and more of a targeted discussion on select deep dive topics. I also put together a pre-survey for the next iteration of the QCR, asking participants to choose from a selection of topics for deep dive.",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "Tell me about a time when you planned and began a research project, but then somehow decided to alter your research approach, or added another phase to the research in order to make improvements to your initial data collection. How did you decide to alter your approach, and/or more generally, what factors might you consider when weighing whether to persist in an approach you have taken towards a problem versus trying something different?",
		"answer": "On AWS, I was using MTurk as a tool for conducting research, mostly as a way to quickly source participants at low cost for short study tasks. I was familiar with some of the constraints when working with MTurk… you’re casting a wide net of potential participants, there’s potential for questionable data coming in. To account for that, I built a screener to help narrow down participants which once passed would lead to the study task itself. However, ran into a fair number of issues setting up the mechanical turk tasks and gratuity associated with it…In my first approach, I grouped everything (pre-screener, first click) as one task with clearly stated messaging that you would get rewarded if you pass the screener and completed the first click task. After launch, I got a few responses initially but then it quickly dried out. I was unclear what was happening here and went to the MTurk community forums where to my surprise, the task I had set up was rated poorly (which was why few people were taking on the task). It was mostly how I had structured screener and task together. Participants who didn’t pass the screener felt like they were unfairly compensated for their time. I revised with a screener paid at nominal fee, from that pool, if they passed the screener, I’d open up a second task to participants where I would be collecting the actual relevant data.",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "Have you ever introduced a new methodology that you needed to get buy-in from stakeholders to pursue? Describe how you did that.",
		"answer": "I was working with PM on a project and one of the open research questions was around the impact of pricing on packaged services. The stakeholder didn’t have a strong formulation of what the research question entailed and so I worked with him 1:1 to understand the problem space. It was mostly an informal conversation; I wanted to understand what he was thinking of for this project, the assumptions behind his thoughts, the users that would be impacted, and the journey he would envision them going through. Over the course of the conversation, it was clear that he struggled with with some fundamental questions on how to position a set of packaged services and at what appropriate price. This didn’t fit well for a traditional qualitative study and I talked him through the limitations that I saw. When it comes to pricing, users lean towards lower prices (why wouldn’t they?) A qualitative study would only confirm a pretty obvious insight while not getting the granularity needed to inform his decisions. What’s more interesting is price sensitivity, understanding how a user’s preferences might change as a function of the price and components included in a package. A better fit for this type of question is a conjoint study; we randomize elements of a package across different price levels and users are forced to choose amongst 3 or so packages. By repeating this across random packages, we can build a model for how users balance tradeoffs in making a decision. This approach was novel to the PM and rather than talking about it abstractly, I pulled up an existing project where I had leveraged the same approach. I was able to show him an example of the outputs such as how preference decreases as price increases (unsurprising), however there are inflection points where the drop isn’t as significant. This would represent price points to consider. I was also able to show how different components of a package have differing impact on overall preference. Having this concrete example helped PM understand the approach and they were fully on board with the research.",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "Think of an example of where your research wasn’t impactful. What did you learn from that experience?",
		"answer": "I’ve seen a lot of instances where personas have landed poorly. It’s unfortunate because personas are a significant investment in time, there should be a fair amount of research that goes into informing persona, and an element of storytelling to bring these persona to life. Some pitfalls I’ve seen with persona work fall into 3 areas. Data: First is grounding the persona on data and research insights. I’ve seen feedback from stakeholders along the lines of, well, how should I trust these anecdotes that you’ve created for the persona and so being able to point back to data will build more credibility. Alignment: sometimes I find that we go into creating a persona without a great plan of how it will be used. They are shared with the team and really just sit there, occasionally referenced. What I’ve found helpful is to first meet with stakeholders and understand what their needs are and how they understand the users. I also like to get alignment across the leadership team on these are the particular persona that we consider high priority. Dissemination: sharing out persona is a start. Making sure they become incorporated into the culture of the team is another challenge. Part of it is using the vernacular of the persona regularly, being stewards of ux by referencing back to who we are designing for. I’ve also combined persona with user journeys and held bi-annual meetings with leadership to review the journeys, providing updated scorecards for how we are doing.",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "YHow have you been able to motivate change in processes, or the way work get done - on a UX team level?",
		"answer": "Evidence of ensuring data is solid (e.g. multiple sources), consideration of stakeholders views, care in communications, impact, follow-through. I think change just for the sake of change rarely leads to positive results. I approach this similar to how I would approach a research problem by first understanding the underlying needs and issues that we face as a UX team. I may have some hunches and existing pain points within workflows, but that’s a very one-sided view. I start by first talking to fellow members within my ux team to understand their workflows. I would hate to solve for a problem I face only to add work and disrupt the process of others. When it’s consistently identified as a pain point, that’s when I try to get my team members invested by brainstorming on how we might solve for it. I find this is helpful because it gets the team invested in the change, rather than having a prescriptive, you should now do this as part of your process. This is where the analogy of user testing begins to fall a bit because it’s trickier to prototype different solutions to try out, but the concept of trying and learning remains the same. I would try out the process change with the team and from there get feedback. Has it solved the existing problem? Did it create new problems? Should we continue forward?",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "Researchers have observed that designers often ‘fixate’ on visual or conceptual examples of prior product solutions, a phenomenon sometimes referred to as ‘design fixation.’ Imagine that you are working in a new product development group - what might be some of the concrete steps that you might take in order to minimize the chance that your team will be negatively affected by design fixation?",
		"answer": "Assuming that most candidates will have never heard of design fixation, strong answers here might include:  the avoidance of competitive reviews of other products, the compartmentalization or aggregation of competitive reviews, or design practices that are centered around meeting goals or solving problems (rather than fast follows of other products).",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "Researchers sometimes need to think outside the box in order to help their team succeed. Describe a way in which you have innovated on a standard research method in order to address a specific research question.",
		"answer": "First click testing for AWS menu design. Short time period, unable to A/B test, multiple designs to explore.",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "You are new to a project and there's no persona or customer segmentation information available. How would you go about developing personas that are representative and practical as design tools?",
		"answer": "Applies a mixed methods approach, involves both ethnographic work in addition to surveys, market segment data, usage logs, Understands difference between market segments and personas. Discusses approaches for dissemination, usage of persona",
		"tag": ["tag1", "tag2"]
	},
	{
		"question": "Your eng team is paralyzed as they have so many features they want to build for the next quarter, but only enough resources to do a couple. Design a study to help them prioritize what they should work on, to drive the most conversion.",
		"answer": "Talks through different feature prioritization approaches (Kano, conjoint, max diff, ratings). Includes initial formative/foundation research to identify user needs.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "You are evaluating 3 different UI text options for an app download screen. You can test these options via a series of moderated usability session or via an online survey tool. How do you determine which approach you pick? For the survey based approach, how many people do you end up surveying? Would you show them each of the 3 variants? Only one?",
		"answer": "Ask follow-up questions about goals for testing, state of the UX. Tradeoffs in study design, usability to iteratively improve, identify issues in variants at the cost of time and resources, generates new/unexpected insights, answers ‘why’. Surveys to scale for larger audiences, quantitative comparisons across the designs, Survey sampling and study design (within vs between subject tradeoffs)",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "In usability studies, how would you describe your moderation style? What is your best technique to capture behaviors and reactions most representative of how users will do in the real world setting? Participants don't talk? Participants that talk non-stop? Participants who think they know what you want to hear?",
		"answer": "Focus on behavior, less so on subjective statements. Moderation best practices for moving studies along at a good pace while gathering high quality data. Making sure participants are comfortable, building up rapport. Listen, don't just hear, assume a beginner's mindset, avoid introducing bias in the conversation, paraphase to make sure interpretations are correct, ask about past experiences rather than future predictions, don't ask leading questions or binary responses, use prompts to get users to tell stories (tell me how..., show me your approach..., what makes this challenging...), refrain from explaining, allow space to speak, resist the urge to help, converse don't interrogate, lead from general to specific",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "After launching new designs for a product, your long-time users had a strong negative reaction to the initial designs. Design a research plan to study and understand the reaction. You are particularly concerned with separating out change aversion from core problems.",
		"answer": "Focuses on tracking sentiment and change aversion. Recognizes that there is some time before stabilization. Baseline before and after comparisons with satisfaction tracking. Looks at only new users to control for change bias",
		"tag": ["tag3", "tag4"]
	},

	{
		"question": "Suppose you are joining a relatively young team yet to have the luxury of having a dedicated user researcher. What are some strategies you would employ to help the team better leverage your core competencies?",
		"answer": "Focus on building rapport with team, defining processes, building relationships. Encouraging participation in design activities, research. UXR 101 type of seminar. Regular brown bags to demonstrate impact of research",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Talk through your typical process and timeline for conducting a more tactical usability study? How would this change if you were given half the time to complete the research?",
		"answer": "Explores lightweight usability approaches (cafe studies, remote usability, unmoderated usability, internal studies, advisory boards). Recognizes potential tradeoffs with these approaches",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How would you evaluate a concept for a new product or service that has not yet been seen in the market, in other words there is no direct peer or competitor to compare it.",
		"answer": "Ethnographic research to understand user needs, inform concepts. Competitive review to understand what is currently out there, though caveat being you don’t want to become too fixated on existing designs such that it impedes innovation (ex/ in focus groups, I’ve seen occasions where users often reference other products and features that they found exciting). Concept study to explore early ideas. Trade-off between concept studies and survey based approaches such as Kano/MaxDiff. Concept studies are great at diving deep, though it’s time consuming (creating concepts) and you are often limited in how many you can share. You can use a storyboard to describe the concept holistically within a flow. Surveys allow for greater scale (number of concepts, number of participants and subsegments of users), since these responses are quantified, you can get more nuanced comparisons and rankings of concepts",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "What have you found to be the most effective ways to communicate your research findings and recommendations? Provide examples.",
		"answer": "Range of approaches depending on audience. Domains international research across 5 countries. Crafted two different presentations depending on audience. First one was to the entire xfnal team (eng, business, leadership, ux). This was a broad audience, and my goal here was to increase awareness and empathy about he international adoption of our product. Decided to do first send out a broader quiz to the team, with questions based on our research findings. Then during the presentation, I showed the results of the survey responses along with the correct answer. This was a great way to demonstrate how the team’s initial conceptions may or may not have aligned with our actual research findings. Added a bonus android figurine for the the top quiz scorers as a bit of motivation also. Second approach was a smaller group of 5-6 stakeholders (PMs, business leads, eng leads). Goal here was different, instead intent was to drive action based on the findings. I reiterated some of the high-level points from the research but I left this presentation more open-ended allowing for discussion and brainstorm of ideas.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Whether or not the idea was eventually adopted, describe a situation in which one of your research efforts resulted in you having an interesting or novel product or feature idea. ",
		"answer": "I was conducting a shop-along type of exploratory exercise to understand how users select domain names for their business. The goal of this research was to generate ideas for iteration of our search process within Domains. I provided users with a re-imbursement, watched them work through the process of finding a domain name. One area that stood out in this walk through was the emphasis users placed on social media handles. For them, a business wasn’t just the website name but also an instagram, twitter identity, business listings on Facebook, Yelp. The underlying need for users was to have a cohesive brand for their digital presence. The idea that came out of this was… could we also check social media availability in addition to domain names as part of the search experience?",
		"tag": ["tag3", "tag4"]
	},

	{
		"question": "Describe a difficult situation you encountered with a product team (e.g. strong opinions and personalities).  How did you deal with this situation?",
		"answer": "On the AWS marketing team, there tended to be some tension between the marketing analytics team (responsible for A/B testing) and the UX team. I received a number of questions about the generalizability of my insights from qualitative work, mostly along the lines of… we haven’t really seen this as an issue in our log data. Underlying this is a question about small sample sized qualitative research. Early in my career I would take a more defensive approach towards it and go into thesis defense mode, talking through the rigor of the research approach. I’ve shifted away from that  and instead start by trying to understand where the log data is coming from. I reframe it as another dimension that can be used to tell a cohesive story. Often, I find that it’s not really in conflict with the qualitative insights and when you dig deeper into the log data, complemented with the qualitative you get a more thorough understanding. I also talk through how the questions being answered by these data sources are different. Logs -> more about how often and what is happening, while qual -> why it’s occurring, the intent behind actions. As an example, let's say you introduced a CTA and the engagement rate is incredibly low… is it an issue of discovery or utility? ",
		"tag": ["tag3", "tag4"]
	},

	{
		"question": "Describe a situation in which your research plan or insights were challenged by stakeholders. How did you deal with this situation?",
		"answer": "Worked really hard on a feature redesign within Google Domains, informed by iterative usability studies, and grounded by early foundational work. As part of the post-launch process, we conducted A/B testing and there were some serious considerations to take into account. Both an influx of negative feedback and a reduction in A/B metrics. Some of this I’ve experienced before in other products and I pointed out how often with redesigns there may be an initial negative reaction. Some of the open-ended feedback related to aesthetics, and though helpful to note should also be balanced by change aversion. One approach I proposed to tease out some of this impact is to actually give the metrics some more time. I also suggested breaking the metrics down into new users vs old users to help control for change resistance (there new users would not be impacted by the old design).",
		"tag": ["tag3", "tag4"]
	},

	{
		"question": "Tell me about a time you had to deliver negative feedback to a team member.",
		"answer": "I was mentoring a researcher on my team and had given him the lead on a research project. During the course of the project, there were a few instances where I didn’t think he communicated effectively to the stakeholders. In the first, the researcher did not loop in stakeholders regularly and so they were surprised when a study announcement was sent out a few days before the first participant was scheduled. This left the stakeholders off-guard and felt unprepared for the research. This led to a delay of the research and a rescheduling of the participants. In the second instance, I noticed that the stakeholders (design/pm) came out of a research meeting somewhat frustrated. In leading the meeting, the researcher was deferring to design/pm to often make research decisions (how many concepts should we test, how should we word these concepts). I followed up with design/pm to verify that the observations were correct before talking with the researcher one-on-one. I framed the feedback in terms of action and impact versus focusing on the researcher themselves. For example, “I noticed that in the meeting stakeholders were often asked about research design decisions. It’s great to include stakeholders in this planning process but leaving it that open-ended places a lot of burden on the stakeholders to also be researchers. Oftentimes they are turning to you as the expert. An alternative approach could be to provide a stronger point of view on what you would recommend given your expertise and if the stakeholders disagree or have alternative opinions, this would then be a great point of discussion.",
		"tag": ["tag3", "tag4"]
	},

	{
		"question": "What are the weaknesses of personas? How do you overcome those weaknesses?",
		"answer": "How are they to be used? Can we get alignment on high priority persona? Personas are a great resource when it comes to providing a frame of reference for designers, developers, product managers. They can all reference back to the persona when trying to frame key decision points. In addition, it creates a realism and sense of empathy when you can refer back by name to the persona, place that persona within a scenario, and walk through how they may perceive of a feature. What goes into creating a persona can often lead to limitations in use. For example, I’ve seen instances where personas are mainly market segments, consisting of demographic profile information. There is no backstory that drives the needs of the persona. On the other hand, I’ve seen personas made with dubious supporting evidence. Despite the story telling nature of crafting personas, these are based on a certain level of evidence. You use a mixture of focus groups, interviews, contextual inquiries, observational studies, market segmentation data to build up an understanding of the user groups. From this you can craft the personas for distinct groups. I’ve found that you get the most out of creating the personas since this involves a fair amount of research and data gathering. As you transition that into the persona story, unfortunately some of the information will get lost. A third party reading the persona, though it may be helpful, loses out on the context driven by persona creation. I’ve found in this case that having designers involved with the persona creation process pays dividends. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How do you ensure that the research your group performs is impactful?",
		"answer": "I approach this from two ends. First is identifying the research that will make a strong impact. As researchers, we have strong insight into when research could benefit the product direction. In interacting with users regularly, we can pull out overarching needs and pain points that aren’t necessarily scoped to a particular feature. I think it’s part of our role to insert ourselves when we see that decisions are made without a strong grounding on user insights. When thinking about what makes a research effort impactful, I look at: what problem is being solved for the users (both in terms of frequency and severity), the novelty or degree of risk associated with the solution, the impact on the business, and level of engineering effort. The second end is what happens after conducting the research. I find that one of the most overlooked parts of being a researcher is the ability to tell a compelling story. Afterall, in the end, you want your work to drive action. I’ve applied a number of different approaches, and it varies very much by audience. For example, after doing some extensive international research: large team wide group I presented as a Q&A type of exercise (what do you think you know, what did we find out). But then in smaller working group, it was focused on highlighting the issues and opening up discussion for how to address them. The goals of the presentation were different. When presenting at a leadership level, I try to understand what questions they currently have of the space, how they respond to different insights (ex/ are they very much focused on data, stories).",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "With what methods do you have experience analyzing qualitative data? (Content analysis methods). Can you tell me about a particularly challenging bit of qualitative data analysis you’ve done in the past?",
		"answer": "I often apply a bottom up qualitative thematic analysis approach. I start by reviewing the data and developing a preliminary codebook which I then go back to code the sessions. From the codes, I extract out common themes. For the full rigor, I would do this with another researcher to collaboratively define the codebook, conduct the coding independently and then check for inter-rater reliability. In practice, this is often too time consuming and it’s just a one-person approach. I ground my insights on user behavior and needs rather than purely what users say. Often times users provide feedback such as, oh I like design A over B, but what’s more interesting is diving into why, what needs are being met/unmet by the designs? And their behaviors which they often don’t realize they are doing. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "What research methods are you most expert in? What are their strengths and limitations compared to other methods?",
		"answer": "My strengths are focused on applying mixed methods approaches towards research questions. I get most excited when I get an opportunity to blend both quant and qual work in order to provide a more holistic understanding of a research area. Some examples of this include: persona development for domains (quant: log and segmentation analysis, qual: user interviews), GMB product integration (quant: log analysis, qual: user interviews), feature prioritization studies (quant: maxdiff/Kano, qual: concept study). For me it’s finding the right approach towards answering a research question. I use the quant approach often to understand existing behaviors within product (log data), or to scale responses for more targeted comparisons (survey). The qual methods provide me with greater depth and understanding of user needs. There’s often an interplay between the two approaches; for example I’ve used interviews to then inform surveys.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Think about an app you like to use. Suppose the product manager tells you that he wants you to find the top 10 UX issues. How would you go about this?",
		"answer": "First start is to set criteria for what is defined as a top UX issue. Does this include the number of people impacted, how much it impairs task completion, impact on user perceptions, business ramifications? Once that is better defined, then we have a more consistent frame to compare issues. Next I would look at the different data sources we have available for user insights. Often UX teams already have a repository of research results, there’s also customer support tickets, and maybe even open-ended feedback in product. I would dig through that to start grouping and identifying themes for UX issues. At the same time I would also think about the inherent biases in these sources. For example, those who submit support tickets may face a different level of severity for their issue, vs someone who leaves open-ended feedback. And both of these groups often represent a particular subset of users who contacted our product. To me, these provide different lenses into user issues and that was why it was important to have a rubric ahead of time to define what it means to be a significant issue. Once the themes have been identified, I’d use the previously established rubric to start ranking the issues. In the case that I was coming into it a complete vacuum with no preliminary data then I would consider this a more long term project. You have to get that initial set of studies (plural) going to see the big picture issues that come up. Once I have a preliminary list of issues, I’d design a research study to validate them. This could be in the form of a prioritization survey such as Kano, MaxDiff. If the list is small enough, I might consider a qualitative study with artboards demonstrating the issues on the list. Then I would follow-up with questions around whether this is an issue they’ve faced and how impactful of an issue it might be for them. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How do you handle it when people are skeptical of the value of usability research?",
		"answer": "I come across a number of articles related to this. One good resource is the Medium article ‘The Secret Cost of Research’. Her main point is that it gives the different members of a project a shared basis for decision-making, grounded in evidence rather than in authority or rhetorical appeal. Coming from a health informatics background, the example that always comes to mind for me is the Cedars Sinai implementation of a computerized physician order entry system. Great team, brilliant work, implemented this system for physicians to input drugs, automatically detect adverse interactions -> rolled back after 2 weeks as physicians refused to use it. Too many false alerts, did not align with their workflow, complexity of the system. Couple approaches to increase buyin… have people observe the sessions. It’s amazing the feedback I’ve received as stakeholders sit in for the first time and get a chance to see users interact. I also try to speak of it from a business perspective. Test early and you are able to identify key issues without sinking resources into development. You also get to try out different iterations. It also always goes back to the user. Having a great user experience will improve your key performance indicators, whatever they may be.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Under what situation would you use focus groups? Usability sessions?",
		"answer": "Focus groups consist of a small group of people sitting together discussing their thoughts and opinions on products, past experiences with them, reactions to new concepts. Really good for getting a sampling of users’ feelings and opinions about things. Good for abstract wants, needs, and likes (is the idea, value prop on the right track?) However it’s no good at learning whether or not your product works and how to improve it. Usability tests involve watching one person try to use something so that you can detect and fix things that confuse or frustrate them. Main point here is you are watching as they use the product rather than listening to them talk about it.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How would you determine a metric for engagement?",
		"answer": "I would first start off by teasing apart what it means to be engaged. Always hesitant to jump right into defining these constructs, especially since there is a subjective, qualitative component to them. My thoughts on engagement are that it involves both depth and quality of interaction with your feature of interest. One way we could validate this is to interview users, have them walk through a few apps, sites that they find engaging. Dig into what makes it engaging. I know for clickstream data, engagement gets thrown around a lot as number of people who view a page and don’t bounce off, or stay for x period of time. Always a bit critical of that approach since you don’t know if they are on there because it’s interesting, fun, or if they are frustratingly trying to find a piece of info that doesn’t exist. A couple ideas come to mind for measuring engagement. One would be a questionnaire approach. In that case I would go to the literature to see what people have developed previously to quantify this construct, check that it has been validated. I could consider coming up with a questionnaire of my own… but that’s a fair amount of work b/c you want to make sure you are measuring the right thing in a consistent manner and interpreted properly by respondents. Alternative approach is a more qualitative descriptive. Review usability sessions, code it for engagement. Maybe double code for inter-rater reliability.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Asked if the team want to add a feature, what research process and activities will I do, what questions I will ask for the usability testing etc..",
		"answer": "Start by understanding where this request is coming from. What potential problems prompted the decision to incorporate the feature, what is it intended to solve, has there been existing research that informed this, perhaps as a baseline of some form? What about the audience, are you targeting a certain group, is the impact across all user types for your product, perhaps it’s a feature for advanced users? Are there any preliminary concerns with introducing the feature? What do you consider a success? Once I understand the needs from the stakeholders along with the business requirements, then I will look at the study design. This really depends on the questions that stakeholders have. If it’s more exploratory and generative in nature, I would consider doing observational work, contextual interviews, or lab studies to see how users are currently interacting with the product. If that leg work has been completed and we’re at the stage of ideation and iteration, I would look at doing a usability study to inform the design, make sure we are going in the right direction without investing significant resources into development. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "They asked me to define some metrics for measuring fun and satisfaction for a mobile maps product. How would I collect the data etc...",
		"answer": "Initial thoughts on this are that, fun and satisfaction are challenging metrics to quantify. There’s a personal subjective element to them. For example, different individuals have differing thoughts on what is fun. The first approach for me would be to clarify what these constructs refer to, both from a stakeholder perspective and an end user perspective. One approach to doing this is an interview with users as they try out some of their favorite apps. I would want to dig into, over the course of the interview, what makes the app fun and satisfying. For example, is it due to elements of lightheartedness, is it because they are able to accomplish a goal quickly, is it because they discover secondary benefits while completing a task? That gives insight into the underlying elements that define fun and satisfying. I know there are clickstream data approaches available to get at some of this information. For example, I’ve seen people measure duration on a page as a proxy for engagement, or completing a task such as sign-up successfully as a measure of satisfaction. I’m always a bit hesitant about reading into that heavily, primarily b/c you lose context for why users are interacting in such a way, in particular you don’t have much insight into their intent and their reactions afterwards. I would take a mixed approach, first qualitative in nature where I have them use the product across a set of open-ended tasks. I would later go back an qualitatively code for instances of fun and satisfaction based on a rubric. I would also look at administering a questionnaire at the end of the session designed to measure these constructs. For the questionnaire approach, rather than coming up with a set myself, I would go to the literature to see what has been published. These tend to be validated in some way, though as a secondary resort I would design the scale on my own.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "The only one that stumped me was what's the difference between a Persona and a Market Segment.",
		"answer": "Market segments refer to different groups of customers based on predominantly demographic, quantitative data. It’s an approach where you gather characteristics of your customer, use it to target different products, services, or messages. It describes the composition of your users. Personas integrate both market data along with interviews, focus groups, usability studies, (more qualitative approaches) to define a profile of the customer. This profile contains a story that sets up the context of the persona, their motivations, needs, goals. It’s a fictional character(s) rather than an averaged group of customers. Personas are used predominantly to assist in the design process as they help designers empathize with the customer story. Designers are not necessarily customers; personas help provide this contextual switch. Personas also don’t have to represent all of your segments.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How I would protect a user's content if they chose to use their personal email during the experiment?",
		"answer": "For those types of situations, I would want to avoid altogether in the first place if possible. For example, if over the course of a study, I know that login may be needed, I will provide users with login credentials as part of the instructions. That way we don’t get into the uncomfortable situation of them putting in personal information. There are cases where it might be helpful to have them log into their account. For example, in AWS, it helps to understand how users may have set up a view of the console dashboard or to provide an understanding of the architectural setup. In these situations, I am very explicit in making it an optional choice for the user; they are under no obligation to share though the NDA is mutual in case they are worried about confidentiality. As they login, I stop the screen sharing so that they are able to type in the information confidentially, get to where they need to go without disclosing any other information.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Explain the P value as if you were talking to your grandmother.",
		"answer": "Talk about how you are using a sample to estimate the true value of the population. It’s not going to be a one liner unfortunately. In order to understand p-values, I think you need to have context for hypothesis testing. In short you have a null hypothesis, a statement of assumed truth (ex/ men and women are of equal height). You have an alternate hypothesis that you want to test against the null (ex/ men and women aren’t of equal height). You go and collect a bunch of data on heights of men and women. The p-value is the probability that you have such a sample, assuming that your null hypothesis was true. If the p-value is significantly low, you would conclude that, something like that happening by chance is so unlikely that the null hypothesis is probably false.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How do you know when a project is done?",
		"answer": "Good question, it’s never about doing a study sending out the report, holding a debrief and then you are set. Biggest challenge at my current position is the follow-up. We identify issues, some stakeholders are better than others at addressing them… but still there is quite a fair bit of pestering that needs to be done. I’m looking at putting together a ticketing system that catalogues the issues, assigns an owner to them, and allows tracking of the issues. There’s always a balancing act also between level of effort and severity of an issue; have to be persistent about the issues that you find critical. I also think we focus too heavily on the launch of a feature and don’t effectively track landing outside of an A/B experiment during the first few weeks. That’s a bit of a disservice b/c you get great insights into how users’ behaviors have changed after the introduction of a feature that you can monitor with logs, get open ended feedback from. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "tell me how would you design and lead a usability test for a product that is going to be launched in 6 month. give me detailed method and user recruitment plan.",
		"answer": "Start by meeting with stakeholders, be it product managers, developers, designers. Get a better sense of the current state for the product (is it early prototype, under development, pretty close to complete, beta testing for bugs). This makes a difference in terms of defining what the study plan will be. I also want to understand what the concerns are from the stakeholders, any assumptions they are making about the user, what existing research has been completed, what questions do they have about the product. This is usually my kickoff meeting, from which I put together the research questions. Figure out what the target user profile will be. This allows me to put together a participant screener. At AWS I’ve got a few approaches towards recruiting… ranges from different user profiles we have available, intercept requests on marketing pages, and third party screeners. Regardless this is generally the bottleneck and so I would want to get it started as early as possible (2 weeks ahead) . Easiest for me is to put together a screener, send to a third party recruiter. While the recruiting is underway, I would put together the task list while working with stakeholders. Set deadlines and expectations for development/design. In particular, what is the state of the product available for testing. Have a review of the task list, make sure everyone is on the same page, put time on people’s schedules for the testing. I recognize that people’s schedules are busy, but set the expectation that they observe at least half of the available sessions. As prep for the study, I would do a pilot run if time allows. Iron out kinks for task list, identify any gaping holes for product functionality, also make sure all the logistics of the study are set (equipment, recording, audio…). Study week itself isn’t too bad, probably one of the more enjoyable stages since you are interacting with users. Hold a short debrief after each session, ask observers about some of the issues they saw, make adjustments on the fly if it’s consistent. ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Take me thorough how you do descriptive/inferential analysis for quant data?",
		"answer": "My first thought is to understand the type of data we have, the questions we’re looking to get answered, and whether statistical analyses are appropriate for addressing them. For example, if it’s small n data, richer in qualitative insights, I don’t see value in focusing on statistics for it (or even really reporting the N), instead I focus on ‘some’, ‘most’, ‘all’ descriptors. In the case where it is yes, next step is to figure out is the data that we are collecting (ordinal, categorical, continuous). Followed by the study design and research questions. For example, is it a within subject study, is it between subjects. What are the hypothesis being tested with the data. This makes a difference later on in the choice of statistical tests, paired vs unpaired, while the data type makes a difference on parametric vs non-parametric approaches. In short for me, choosing a statistical test is all about understanding the assumptions made of the test and whether or not your data meet those assumptions. The other issue I try to catch is over testing and cherry picking. This is especially true for web optimization related tests where you have a fair number of metrics available. The issue is that, if you look around for metrics of significance here and there without predefined objectives, you open yourself up to false positives. As a result, I make sure that we have hypotheses defined before hand, with corresponding metrics to test and desired power. This defines the sample size and a stopping point for the tests.I don’t do anything too fancy when it comes to testing. Truthfully I always forget the names of the tests but that’s where a quick Google search helps. My decision point is: descriptive vs inferential, parametric vs non-parametric data, paired or unpaired data, and comparisons between 2 groups or amongst more than 2. The ones that come up most frequently for me are t-tests (paired, unpaired) for continuous data, ANOVAs for comparing across multiple groups, Chi-squared for multiple for groups. For non-parametric data, go with Chi-squared, Mann-Whitney, Wilcoxon (paired). ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "You are the UX researcher of a large-scale website that offers news to a multi-national audience. You are now asked to conduct research to find out if there are regional differences in user satisfaction, and to recommend for which region to improve the website.",
		"answer": "I’ve found that there are often cultural differences across regions impacting user satisfaction. One approach I would consider is to look at satisfaction over time within different regions and demographics. I want to control for some of these differences across location and segments, given that there are many cultural differences in play. I would also do external research to understand differences in satisfaction across geographic locales. Mostly what I’m looking to separate out is whether differences in satisfaction are due to the product or if there is a difference in baseline satisfaction ratings based on cultural influences. I’d then follow-up with interviews across a selection of the regions to understand why these satisfaction differences exist. My recommendation coming back to stakeholders would be based on the severity of the issues identified, number of users impacted, and frequency of the issue.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Imagine you’re working at a company that builds products to promote healthy living. Describe the research would you conduct to help the team decide what the opportunities are in this space?",
		"answer": "First sync with stakeholders to better understand the goals of the research, what open questions they have, what they define as a high priority opportunity, who the target users are. For such an open question, I’d start by looking at existing data, ex/ competitive review, feedback with received in product, customer support calls, sales calls. All these provide dimensions of the user voice. I’d also plan for foundational research to understand what healthy living means to users, how they go about maintaining healthy living, and barriers that impede healthy living. I’d be open and flexible to approach depending on the research questions and constraints (timeline, resourcing). Consider doing contextual inquiries to observe behaviors over a period of time and follow-up with interview questions. Focus groups also provide an opportunity to understand the current landscape and to get multiple users perspectives at once, while also allowing them to discuss and bounce ideas off of each other. I’d use the insights from the qualitative research to inform questions for a survey that would then help me understand how these issues generalize. For example, I could use a survey approach to see how often particular issues come up for users, the severity of these issues, and how they might prioritize different opportunities for solving the issues.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "How would you go about organizing a {RITE, co-design, sprint} workshop? From your experience, what are common pitfalls?",
		"answer": "I held a ux sprint when I first joined the Domains team to ideate on our next iterations of the search experience. A sprint is a significant time investment for all stakeholders and I did a lot of legwork to even identify if a sprint was needed. This involved talking to stakeholders, getting their understanding of where we are in the product, what we currently know of the search experience, and what we would want to get out of the sprint. I made sure to first understand the problem: our search experience has been pretty static, there’s a desire to ideate and expand on that experience, but we don’t have clear direction nor alignment on what to explore. For the sprint, I started by highlighting the goals and intended outcomes, explained why we had the different stakeholders in the room (to provide a perspective based on their knowledge of the users or of the product). I then wanted to align the stakeholders on what we currently understood of the search experience and so had an initial shareout from XFN members (they ranged from eng, ux, and customer support). The intent here was to lay the groundwork across different lenses that interact with the search experience. How did we get to this point and what do we know of how users interact with search. As the presentations were going, I had participants jot down HMWs… This helps generate early ideas while framing it in the context of a user problem. We did a shared affinity mapping exercise to group the HMWs together into larger themes. I then set out to have participants generate solutions for these HMWs through a crazy-eights exercise (here I was going for breadth of different ideas through brief sketches). We mapped these ideas back to the HMW themes that we had identified earlier. To scope down these ideas, I held another brainstorming/affinity mapping exercise, here geared on what we define as success for the search experience? I wanted to use this as a shared rubric for prioritizing the crazy eights brainstorm. From there, I asked participants of the sprint to vote on the various crazy eight ideas, select the top 3 based on the shared rubric that we had put together. This narrowed our ideas to a subset for further exploration. Next was getting some form of validation from users. Here I worked with designers to translate the ideas into storyboard sketches. I like the use of storyboards for concept testing because it is able to represent context and a journey (what the problem is, what the solution ideated is) rather than focusing purely on a feature. I had recruited participants ahead of time for concept testing and so was able to turn around the research pretty quickly. Sprint participants observed the study sessions live and we debriefed after each session. This created a sense of ownership in the work and when the results were synthesized, insights didn’t catch people off guard. The outcome from this sprint was a set of ideas that we were able to narrow down and refine. It ended up leading to two features within our search experience, introduction of domain name insights and the ability to favorite and save domains for later.",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Talk through how you would scope a research study design.",
		"answer": "Scoping the research question. Who is the audience? Some dimensions to consider include demographics, technology expertise, new/experienced with service, geo location, education level. What is the form factor? Are we talking about desktop, mobile/tablet? App? What existing research has been done? Personas, site metrics, user feedback? All of these can help to triangulate your insights. What are the stakeholders’ underlying assumptions about the user and their interactions? Hold a preliminary workshop w/ PM, designer, BDM, marketing manager, developer to get at these assumptions. Are there existing business requirements or limitations? What are some of these business goals? What are the core scenarios in this experience? From the perspective of the PM, business, user? Are there artifacts available to evaluate? What stage is the design in?",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "Describe a situation where you have successfully changed the prevailing decision, or plan, based on successfully communicating the results of your research.",
		"answer": "GMB integration ",
		"tag": ["tag3", "tag4"]
	},
	{
		"question": "If your research revealed that a product was very much on the wrong track, what steps would you take to ensure that it got on the right one? If that failed, what would you do next?",
		"answer": "Evidence of ensuring data is solid (e.g. multiple sources), consideration of stakeholders views, care in communications, impact, follow-through. I don’t have a great example of this that I can share but I can talk through how I would approach the situation. Starting point, triangulate on the data, make sure that I can make a strong recommendation backed by evidence (preferably across multiple sources). Be cognizant of the stakeholders when communicating, disassociate away from the stakeholder’s idea and focus more on the user insights, how it fits within the user’s journey, make it a collaborative environment. Often an idea is based on good intent and I would work with the stakeholder to brainstorm. Don’t just leave it as, this is wrong and it won’t work, I want to provide recommendations or work with stakeholders to address where the limitations are. I’ve found that stakeholders respond differently, and so understanding who I am speaking to, how I should focus the story I tell… is it predominantly from data, is it using the data to tell a narrative, attest to empathy?",
		"tag": ["tag3", "tag4"]
	}
]